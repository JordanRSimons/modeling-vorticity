# Introduction

My goal in this project is to test an ocean model called Lagrangian Gradient Regression (LGR), which computes a measure of rotation in currents called vorticity from solely particle trajectory data (Harms et al. 2023). In this notebook, I will discuss how the test particle trajectories are initialized, as well as how the data is prepared in order to make plots (shown in [notebook 1](https://github.com/JordanRSimons/modeling-vorticity/blob/main/notebooks/01_visualizations_and_results.md)).

Before generating test particle trajectories, I need to initialize particles. With the curved simulated coastline, a simple grid is not sufficient for particle seeding: Instead, particles should be seeded along contours of ocean depth. 

For context, here is the final distribution of the particles' initial positions.

![Seeding Diagram](../images/seeding.png)
*A plot highlighting the starting positions (red dots) of 1200 seeded particles in a simulated coastal environment. The off-white color represents land. This notebook discusses the details of this setup process.*

In this figure, the white represents a landmass, and the color scheme represents water depth in the idealized simulation. The 1200 red dots represent the seeded particles - note how they appear on top of black contours of equal water depth. Code for this figure can be found at the end of the notebook.

# Seeding Particles with Flexible Functions

The Python code to seed particles needed to be flexible enough to allow quick testing of a variety of seeding schemes (specifically I want to minimize particles used and maximize model accuracy), so this process is coded as a pair of functions. Broadly, the first function generates contours of equal depth, and the second function outputs the desired number of test points distributed between the contours from the first function's output.

I will use the following libraries and a multidimensional NetCDF dataset containing information about the simulated coastline's water's depth and motion:

```python
import xarray as xr
import numpy as np
import zarr
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import matplotlib.animation as animation
import matplotlib.colors as colors
import cmasher
import pandas as pd
import cmocean
from scipy import stats
from scipy.ndimage import median_filter, uniform_filter

ds = xr.load_dataset('/Users/jordan/Documents/CICOES/data/cape_large_00.nc', decode_times = False)
```
## Function 1: getContours

```python
def getContours(ds, levels = False, ncont = False, bound = 10) :
```
This function takes four inputs, supporting two distinct methods of contour creation:
* ds: imported above, a dataset related to the simulated basin
* levels: an array of specified depths at which to make contours
* ncont: the number of contours to include (only used if levels=False)
* bound: the maximum depth to draw contours at

The output will be two arrays, X and Y. Each term of X is an array containing all the x positions of one of the requested contours. Y contains the corresponding y values.

If levels are not provided, they are generated at even spacing:

```python
    # determine contour levels, if needed
    if levels == False :

        # exclude the right endpoint
        # compute one more value than necessary so the left endpoint can be removed as well
        # we don't want particles seeded at a depth of 0
        levels = np.linspace(1, bound, ncont + 1, endpoint = True)
        levels = levels[1:]
```

The contours themselves are actually generated by creating a matplotlib plot object. The plot serves as a diagnostic tool as well, but I will not display it here.

```python
    # generate plotting grid based on the rho points (positional coordinates of a special form)
    x_rho = ds.x_rho[0,:].values 
    y_rho = ds.y_rho[:,0].values
    Xplot, Yplot = np.meshgrid(x_rho, y_rho)
    
    fig, ax = plt.subplots(figsize = (5.5,4), dpi = 170)

    # create depth contours based on ds
    # note that contours are stored for later use
    conts = ax.contour(Xplot, Yplot, ds['h'], levels = levels, colors = 'k')

    # boundary contour
    ax.contour(Xplot, Yplot, ds['h'], levels = [bound], colors = 'red', linestyles = 'dashed')

    # draws the land
    ax.pcolormesh(ds.x_psi, ds.y_psi, np.ma.masked_where(ds.mask_psi == 1, ds.mask_psi), cmap = 'gray')
    
    # axis and plot attributes
    ax.set_xlabel(r'$x$ (m)')
    ax.set_ylabel(r'$y$ (m)')
    
    ax.set_ylim(0,1200)
    ax.set_xlim(-1050,1050)
    ax.set_aspect("equal") # make a meter the same distance on each axis

    plt.title("Bathymatry Contours Output: {}".format(len(levels)))

    plt.show()
```

These stored contours can then be accessed to give the desired points. 

```python
    X = []; Y = []
    
    for i in range(0, len(levels)) :

        # take the ith level, 
        # then get an array of the x values and y values making up matplotlib contour objects
        xi = conts.allsegs[i][0].T[0]
        yi = conts.allsegs[i][0].T[1]
        

        X.append(xi)
        Y.append(yi)

    return X,Y
```

I now have points on contours, but there is no way yet to control over how many points get included on each contour. In a scientific context, I want equal numbers of particles on each contour, spaced an even distance apart. 

## Function 2: getPoints

For simplicity, say we want 50 points spread across 5 contours (so 10 points per contour). You might think that the x-axis could be divided evenly into 11 chunks, and place a seeded particle as close to each of the 10 dividing dividing lines as possible on each contour.

The problem with this implementation is that spacing particles evenly with respect to the x-axis is not the same as placing them evenly across the contour. Since the contours are curved, distances along them need to be computed differently. The getPoints function performs this computation.

```python
def getPoints(ds, X, Y, npart) :
```
The first three inputs should be familiar: the dataset about the basin, and the two output arrays from the getContours function. The last input, npart, is simply the desired number of particles to be seeded. For the plot at the beginning of the notebook, npart = 1200.

The function then outputs two arrays, xvals and yvals, which each have length npart and contain the x and y coordinates for the seeded points.

To place particles at even points along the contours,I will need the total length of each contour. The distance between any two points $(x_1, y_1)$ and $(x_2, y_2)$ can be approximated via the Pythagorean Theorem:

$D = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$

By adding up the distances between all of the many points that define contour object, I can get a total length, saved in this case as a cumulative sum:

```python
# empty lists for particle position coordinates
    xvals = []; yvals = []
    
    # loop through contours 
    for c in range(len(X)) :
    
        # remember, each term of X and Y is an array containing 
        # all generated x and y values for an entire contour
        xc = X[c]
        yc = Y[c]

        # this code generates an array of distances between particles
        # start at 0 because the first point is the start of the contour, a distance 0 along it
        # S is defined as a distance coordinate along the contour.
        deltaS = [0]

        # loop through all particles (minus the first one)
        for p in range(len(xc) - 1) :
            deltaS.append( round(np.sqrt( (xc[p+1] - xc[p])**2 + (yc[p+1] - yc[p])**2), 3) )


        # this generates a cumulative sum array, where each entry is the value S at point i
        Scumsum = np.cumsum(deltaS)
```


With this cumulative sum, I can then derive an array of increasing distances from the left side of each contour, called Svals. The np.linspace command which derives Svals evenly divides the length of each contour (Scumsum[-1]) into npart/ncont + 1 parts. The total number of particles are divided between ncont contours, and the +1 allows us to remove edge values.

```python
        # the number of contours is the number of separate arrays of positions in the array X
        ncont = len(X)
        
        # exclude the right endpoint, compute one more value than necessary so I can remove the left endpoint as well.
        # don't want a particle at S = 0 or S = Scumsum[-1] because it would be on the edge of the screen.
        Svals = np.linspace(0, round(Scumsum[-1]), int(npart/ncont + 1), endpoint = False)
        Svals = Svals[1:]

        # interpolate the x and y values at the evenly spaced Svals
        xvalsc = np.interp(Svals, Scumsum, xc)
        yvalsc = np.interp(Svals, Scumsum, yc)

        # add the new points to the masterlists that will be returned
        xvals = np.concatenate([xvals, xvalsc])
        yvals = np.concatenate([yvals, yvalsc])

    return xvals,yvals
```

The starting positions have been set, and I can now use the flow data contained in the dataset to produce test particle trajectory data to which LGR can be applied.

Particle seeding for any configuration can now be accomplished easily:

```python
X,Y = getContours(ds, ncont = ncont)
pointLists = getPoints(ds, X, Y, npart = 1200) 
```

# Generating Particle Trajectories

The particle trajectories are generated using a vector field of current strengths originating from wind-driven waves. This earlier simulation  of these current strengths using the ocean model ROMS was done by my research mentor Dr. Torres and is not recorded here. The results of this simulation are stored in the netCDF dataset ds used already in this notebook.

With the stored vector field data, I compute Lagrangian velocities (aka velocities with respect to particles themselves) at all points in the simulated environment. The particles will then follow tracks in this environment.

```python
ds["ubar_lagrangian"] = ds["ubar"] + ds["ubar_stokes"]
ds["vbar_lagrangian"] = ds["vbar"] + ds["vbar_stokes"]
```

The Ocean Parcels code then requires a variety of parameters...

```python
# positions, distinct from the rho coordinates used above
x_psi,y_psi, = ds.x_psi, ds.y_psi
mesh = "flat"

velocities = {'U': 'ubar_lagrangian',
              'V': 'vbar_lagrangian'}

dimensions = {'U': {'lon': 'x_u', 'lat': 'y_u'},
              'V': {'lon': 'x_v', 'lat': 'y_v'}}

fieldset = FieldSet.from_xarray_dataset(ds, variables = velocities, dimensions = dimensions, mesh = mesh, 
                                        time_periodic = False,
                                        interp_method={
                                        "U": "freeslip",
                                        "V": "freeslip",
                                        },
                                       )
```

and then produces the trajectories, saving them into a .zarr file:

```python
xpoints = pointLists[0]
ypoints = pointLists[1]

# function to remove particles if they move onto land between timesteps
def CheckOutOfBounds(particle, fieldset, time):
    if particle.state == StatusCode.ErrorOutOfBounds:
        particle.delete()

### parcels code to output a .zarr file of advected particle trajectories

# feed in the above parameters
# define the x and y coordinates as longitude and latitude
pset = ParticleSet.from_list(fieldset = fieldset, pclass = JITParticle, time = ds.ocean_time.values, lon = xpoints, lat = ypoints)

# name output file
output_file = pset.ParticleFile(name = f"/Users/jordan/Documents/CICOES/data/posterData/1200p_{ncont}c_dt15", outputdt = timedelta(seconds = 30))

# use the standard advection kernel and the CheckOutOfBounds kernel above
kernels = AdvectionRK4 + pset.Kernel(CheckOutOfBounds)

# execute the advection over runtime with steps of size dt
pset.execute(kernels, runtime = timedelta(hours =  1), dt = timedelta(seconds = 15), output_file = output_file, verbose_progress = True)
```

These created files can then be used to compute vorticity.


# Vorticity Computation - Code Walkthrough

This section will go into technical detail about how vorticity is calculated using both LGR, as well as the established ocean model ROMS, which I will use as a point of comparison.

First, the ROMS vorticity is actually contained in dsCDF already. It is computed directly from the simulated water speed. 

Mathematically, vorticity is defined to be the curl of a flow field: $ \omega := \nabla V $ 

In other words, vorticity mathematically quantifies the degree to which a vector field (like in Plot 1) circulates around any given point. Since there is already a full flow vector field in the simulated environment, ROMS easily performs the needed computation, and I can just read in the output.

```python
romvort = dsCDF['Ï‰_bar'].values
```

The LGR vorticity computation relies on a slightly altered version of the LGR model developed by Harms et al., the original proprietary code for which I cannot share here. 

I import their model code:

```python
from LGR_altered.lgr import *
from LGR_altered.jacobian import *
from LGR_altered.classes import *
from LGR_altered.plotting import *
```

Before the computations run, there is some setup required. The first step is to define the regression method. For this analysis, I use radial Gaussian regression.

```python
# Generate the regression function
regfun = setRegressionFunction(kernel=reg_type, lam=lam, sig=sigma)
```
Next, the data from dsTRAJ (the trajectory data from the previous notebook) is used to create an array called particleList, which catalogues each particle as a SimpleParticle class object with position and time data as attributes. The other parameter kNN determines how many neighboring particles to use in the regression. I set kNN = 5.

I also obtain a count of particles.

```python
# Generate a data frame
df = generateDF(particleList, kNN)
n_particles = len(df['indices'][0])
```

Now, the full model is runnable.

```python
# Perform the regressions
calcJacobianAndVelGrad(df, regfun=regfun)

# Compute the metrics on each particle trajectory
# The primary metric of interest is LGR vorticity
computeMetrics(df, t, metric_list=metrics)

# drop the last row as it is prone to errors
df = df[:-1]
```
In a rough sense, what these functions do is record the direction of motion of every particle at each time step relative to the five nearest other particles, weighted by distance, and from these rates of change compute vorticity, the amount of relative rotation. 

Mathematically, at every point, my goal is to compute vorticity in the same way as with ROMS, using the formula $ \omega := \nabla V $. However, starting from just trajectory data, I don't have the necessary vector field V. Instead, I approximate it via a complex process using Gaussian-weighted regression. As time progresses one small step, I track the change in distances between each particle and its 5 nearest neighbors. Regression then gives a matrix which best transforms the old positions to the new ones, an approximate flow matrix. At any given timestep, a composition of these flow matrices gives an approximation of $\nabla V$, allowing vorticity to be approximated.

Continuing to set up the data for statistical analysis, the LGR vorticity data needs to be interpolated back onto the simulation's gridded coordinates, used by ROMS.

```python
# this generates the meshgrid from the x and y values of the LGR model grid
# make the grid sparser for efficiency
xvec = dsCDF.x_psi[0,:].values 
yvec = dsCDF.y_psi[:,0].values 

gridvectors = [xvec, yvec]

# choose interpolation method based on the number of particles
if n_particles < 2000:  
    generateFields(df, gridvectors, approach='rbf', method='multiquadric', smooth = smooth)
    interpstr = 'rbf_mq'
else:
    generateFields(df, gridvectors, approach='interp', method='cubic')
    interpstr = 'int3'
```

The critical parameter which I added to the generateFields function was the smoothing parameter. When interpolating data to fit a grid, especially when the computed data is concentrated only around where the particles currently are, requires the computer to "guess" at how to fill in the empty regions. Adding the smoothing parameter greatly reduces extraneous values in the corners of the plot.

I can then extract the LGR vorticity, called vort in this code. A raw unfiltered copy is made, vort_nofilter, which is the data variable in the LGR panel in Plot 3.

```python
### computed quantities
# .loc selects a timestep, and the scalarfields column. Each element is a dictionary, so pull out the desired quantity
# Each dictionary contains a 2d array array, horizontal values in rows, vertical values in columns, used for plotting
vort = np.squeeze(df.loc[tstep, 'ScalarFields']['vort'])
vort_nofilter = np.copy(vort)
```

Finally, to make the ROMS vorticity data be more similar in form to the interpolated LGR vorticity data, I "smooth" it as well, by replacing each value on the plot with the local mean of the surrounding 5 by 5 box of values, along with some operations to handle some edge cases NaNs.

```python
# where romvort is nan, plug in 0, otherwise keep the original value
# this will prevent NaNs from interfering with the local means
romvort_nonan = np.where( np.isnan(romvort), 0, romvort)

# a grid with NaNs as NaN, and 0s elsewhere
romvort_nans = np.where(np.isnan(romvort), np.nan, 0)

# take the local mean at every point
romvortmean = uniform_filter(romvort_nonan, size = 5, mode = 'nearest')

# create a version of romvortmean where every nan value is put back in place after the computation
# number + NaN = NaN
romvortmean_nans = romvortmean + romvort_nans
```

The romvortmean variable is the data variable in the ROMS panel in Plot 3 in [notebook 1](https://github.com/JordanRSimons/modeling-vorticity/blob/main/notebooks/01_visualizations_and_results.md).

# Statistical Analysis - Code Walkthrough

We are now in a position to explain the details behind the statistical analyses in Plot 4 from [notebook 1](https://github.com/JordanRSimons/modeling-vorticity/blob/main/notebooks/01_visualizations_and_results.md).

The errors themselves are computed via the mean L2 norm, or root mean squared error (rmse). The ROMS data is treated as the theoretical, correct, value while the LGR data is considered experimental.

The equation for the computation is as follows:

$ \text{rmse} = \sum^n_{i=0} \sqrt { \frac{(\text{LGR}[i] - \text{ROMS}[i])^2} {n} }$

I take the differences, square them, take the root, and take the mean (by dividing by n, the total number of points), hence the name root mean squared error.

```python
errorMesh = (vort - romvortmean_nans)**2
meanl2norm = round( np.sqrt(np.nansum(errorMesh)/ np.count_nonzero(~np.isnan(errorMesh))), 5) 
```

This code performs this computation efficiently by creating a mesh with the error at every point then summing the full mesh and dividing by the count. 

For a variety of configurations, I performed this computation, as well as a similar computation of correlation, and recorded the results in a csv.

```python
dsError = pd.read_csv('/Users/jordan/Library/.../CICOES/data/errorData.csv')

# filter out negative correlation values and the ncont = 600 case
ds_poscorr = dsError[(dsError['corr'] > 0) & (dsError['ncont'] < 600)] 

# extract the columns of interest
ncont = ds_poscorr[['ncont']].to_numpy().T[0]
rmse = ds_poscorr[['rmse']].to_numpy().T[0]
corr = ds_poscorr[['corr']].to_numpy().T[0]
```

The final task required to create Plot 4 was to create an error time series for the lower panel.

```python
times = []
rmse_t = []
corr_t = []

# loop over the total number of time steps
for i in range(len(df['positions'])) :
    
    # sometimes the last few datapoints are flawed, want to exit the loop without an error when this happens
    try :
        # .loc selects a timestep, and the scalarfields column. Each element is a dictionary, so pull out the chosen one
        # Each dictionary contains a 2d array array, horizontal values in rows, vertical values in columns, used for plotting
        vorti = np.squeeze(df.loc[i, 'ScalarFields']['vort'])
        vorti[ dsCDF['h_psi'][:,::cff] >= 10 ] = np.nan
    except :
        break
    
    vortveci = vorti.flatten()
    romvortveci = romvortmean_nans.flatten()
    

    errorMeshi = (vorti - romvortmean_nans)**2
    meanl2normi = round( np.sqrt(np.nansum(errorMeshi)/ np.count_nonzero(~np.isnan(errorMeshi))), 5) 
    
    maski = ~np.isnan(romvortveci) & ~np.isnan(vortveci)
    
    corri = round(stats.linregress(romvortveci[maski], vortveci[maski]).rvalue, 3)

    times.append(i*sps/60)
    rmse_t.append(meanl2normi)
    corr_t.append(corri)
```

I have now created all data variables contained in Plot 4.

# Code for Seeding Diagram in Introduction

```python
fig, ax = plt.subplots(figsize = (14,6.5), dpi = 100, constrained_layout = True)

# color the map by depth contour
dep = ax.pcolormesh(Xrho, Yrho, ds['h'], vmin = 0, vmax = 20, cmap = cmocean.cm.haline_r, alpha = 0.5)

# plot contour lines
cs = ax.contour(Xrho, Yrho, ds['h'], levels = np.arange(1,20,2), colors = 'k')
ax.clabel(cs, np.arange(1,20,2))

# plot seeded points
ax.scatter(xvals, yvals, s = 8, c = 'red')

# draws the land
ax.pcolormesh(ds.x_psi, ds.y_psi, np.ma.masked_where(ds.mask_psi == 1, ds.mask_psi), 
              cmap = colors.ListedColormap(['#FAF6EB', '#ffffff00']))

ax.set_xlabel(r'$x$ (m)', size = 24)
ax.set_ylabel(r'$y$ (m)', size = 24)

ax.xaxis.set_tick_params(labelsize=24)
ax.yaxis.set_tick_params(labelsize=24)

ax.set_ylim(0,800)
ax.set_xlim(-1050,1050)
ax.set_aspect("equal")

# create the legend
cbar = fig.colorbar(dep, orientation = "vertical", fraction = 0.05, aspect = 20, shrink = 0.675, pad = 0.01) 
cbar.set_label(label='Depth (m)', size=28) 
cbar.ax.tick_params(labelsize = 24) 

plt.savefig('plots/poster_video/seeding_test.png')
plt.close(fig)
```

